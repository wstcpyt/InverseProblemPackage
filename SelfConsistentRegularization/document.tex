%%This is a very basic article template.
%%There is just one section and two subsections.
\documentclass{article}

\usepackage{cite}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[a4paper, total={6in, 8in}]{geometry}

\begin{document}
\title{Self Consistent ill-posed Inverse Pronlem With Cross Validation and
Cellular Evolutionary Algorithm}
\maketitle

\section{Introduction}
In principle, ill-posed inverse problem with the regularization method can give
out a very good reconstructed result, but in practice, this is not always the
case. The main reason is that we apply our regularization method on the basis of
the given, noisy measured value.

In order to solve the ill-posed inverse problem
which the measure value or the model is not accurate enougth to give out a good reconstructed result, we
introduce the self consistent regularization method With Cross Validation.
\section{Methodology}
\subsection{Fredholm integral equation of the first kind}
\subsection{Tikhonov Regularization}
\subsection{Generalized Cross Validation}
\subsection{Cellular Evolutionary Algorithm}
\section{Numerical Results and Discussion}
We use a very simple model to illustrate how we use regularization method to
solve the ill-posed inverse problem. We use a simplified problem from gravity
surveying. An unknown mass distribution with density \(f(t)\) is located at
depth de below the surface. From 0 to 1 on the \(t\) axis shown in
Figure. \ref{fig:GeophysicsIllustrator}. We assume there is no mass outside
this source, which produce a gravity field everywhere. At the surface, along the
s axis in Figure. \ref{fig:GeophysicsIllustrator} from 0 to 1, we mreasure the
vertical component of the gravity field, which reger to as g(s).
 
 \begin{figure}[h!]
  \centering
    \includegraphics[width=0.7\textwidth]{images/GeophysicsIllustrator/illustrator}
  \caption{The geometry of the gravity surveying model problem: \(f(t)\) is the
  mass density at \(t\). and g(s) is the vertical component of the gravity
  field at s.}
  \label{fig:GeophysicsIllustrator}
\end{figure}
The two functions f and g are related via a Fredholm integral equation of the
first kind. The gravity field from an infinitesmally small part of f(t), of
length dt, on the axis is identical to the field from a point mass at t of
strength f(t)dt. Hence, the magnitude of the gravity field along s is
\(f(t)dt/{r^{2x}}\), where \(r = \sqrt {{d^2} + {{(s - t)}^2}} \) is the
distance between the source point at t and the field point at s. The direction
of the gravity field is from the field point to  he source point, and therefore
the measured value of g(s) is
\[dg = \frac{{\sin \theta }}{{{r^2}}}f(t)dt\]
where \(\theta \) is the angle shwon in Figure. \ref{fig:GeophysicsIllustrator}.
Using that \(\sin \theta  = d/r\), we obtain
\[\frac{{\sin \theta }}{{{r^2}}}f(t)dt = \frac{d}{{{{({d^2} + {{(s -
t)}^2})}^{3/2}}}}f(t)dt\]
The total value of g(s) for any \(0 \le s \le 1\) consists of contributions from
all mass along the t axis (from 0 to 1). and it is therefore given by the
integral
\[g(s) = \int\limits_0^1 {\frac{d}{{{{({d^2} + {{(s - t)}^2})}^{3/2}}}}f(t)dt}
\]
This is the forward problem and writing it as
\begin{equation}
\int\limits_0^1 {K(s,t)f(t)dt = g(s),\begin{array}{*{20}{c}}
{0 \le s \le 1}&{}
\end{array}}
\label{eq:integralequation}
\end{equation}
where the function K, which represents the model, is given by
\begin{equation}
k(s,t) = \frac{d}{{{{({d^2} + {{(s - t)}^2})}^{3/2}}}}
\end{equation}
and the righ-hand side g is what we are able to measure. The function K is the
vertical component of the gravity field, measured at s, from a unit point source
locate at t. From K and g we want to compute f, and this is the inverse problem.

Figure.\ref{fig:geoexact} shows an example of the computation of the measured
signal g(s), given the mass distribution f and three different values of the
depth d.

\begin{figure}
	\centering
		\begin{subfigure}[b]{0.7\textwidth}
			\includegraphics[width=\textwidth]{images/FexactPlot/fexactplot}
			\caption{f(t)}
			\label{fig:f(t)}
		\end{subfigure}
		\begin{subfigure}[b]{0.7\textwidth}
			\includegraphics[width=\textwidth]{images/GexactPlot/gexactplot}
			\caption{g(s)}
			\label{fig:g(s)}
		\end{subfigure}
		\caption{(a) shows the function f(the mass density
		distribution), and (b) shows the measured signal g (the
		gravity field) for three different values of the depth d in Figure.
		\ref{fig:GeophysicsIllustrator}}\label{fig:geoexact}
\end{figure}
\subsection{Discretization of Liner Inverse Problems: Quadrature Methods}
We compute approximations \({\tilde f_j}\) to the solution f solely at selected
abscissas \({t_1},{t_2},...{t_n},i.e.\),
\[{\tilde f_j} = \tilde f({t_j}),\begin{array}{*{20}{c}}
{}&{j = 1,2,...,n.}
\end{array}\]

Quadrature methods--also called Nystrom methods--take their basis in the general
quadrature rule of the form 
\[\int\limits_0^1 {\varphi (t)dt = \sum\limits_{j = 1}^n {{\omega _j}\varphi
({t_j})} }  + {E_n}\]
where \(\varphi \) is the function whose integral we want to evaluate, \({E_n}\)
is the quadrature error, \({t_1},{t_2},...,{t_n}\) are the abscissas for the
quadrature rule, and \({\omega _1},{\omega _2},{\omega _3},...,{\omega _n}\) are
the corresponding weights. For example, for the midpoint rule in the interval
\([0, 1]\) we have
\begin{equation}
{t_j} = \frac{{j - \frac{1}{2}}}{n},\begin{array}{*{20}{c}}
{}&{{\omega _j} = \frac{1}{n},\begin{array}{*{20}{c}}
{}&{j = 1,2,...n.}
\end{array}}
\end{array}
\end{equation}
Apply the quadrature methods on Eq.\ref{eq:integralequation}, we arrive at the
relations
\begin{equation}
\sum\limits_{j = 1}^n {{\omega _j}K({s_j},{t_j})} {\tilde f_j} = g({s_i}),\begin{array}{*{20}{c}}
{}&{i = 1,...n.}
\end{array}
\end{equation}

The realtion in Eq.\ref{eq:disintegral} are just a linear system, which can be
also written as
\[\left( {\begin{array}{*{20}{c}}
{{\omega _1}K({s_1},{t_1})}&{{\omega _2}K({s_1},{t_2})}&{...}&{{\omega _n}K({s_1},{t_n})}\\
{{\omega _1}K({s_2},{t_1})}&{{\omega _2}K({s_2},{t_2})}&{...}&{{\omega _1}K({s_2},{t_n})}\\
 \vdots & \vdots &{}& \vdots \\
{{\omega _1}K({s_n},{t_1})}&{{\omega _1}K({s_n},{t_2})}&{...}&{{\omega _1}K({s_n},{t_n})}
\end{array}} \right)\]

or simply \(Ax=b\), where A is an \(n \times n\) matrix. The elements of the
matrix A, the right-hand side b, and the solution vector x are given by
\begin{equation}
\left. {\begin{array}{*{20}{c}}
{{a_{ij}} = {\omega _j}K({s_i},{t_j})}\\
{{x_j} = \tilde f({t_j})}\\
{{b_i} = g({s_i})}
\end{array}} \right\}\begin{array}{*{20}{c}}
{}&{i,j = 1,...,n.}
\end{array}
\end{equation}


\subsection{The Singular Value Decompostion}
While the matrices that we encountered in the previous section are squre, it can
be aslo apply to general case. Hence, we  assume that the matrix is either
square or has more rows than columns. Then, for any matrix \(A \in {R^{m \times
n}}\) with \(m \geqslant n\), the SVD takes the form
\begin{equation}
A = U\Sigma {V^T} = \sum\limits_{i = 1}^n {{u_i}{\sigma _i}v_i^T} 
\end{equation}
Here, \(\Sigma  \in {R^{n \times n}}\) is a diagonal matrix with the signluar
values, satisfying
\[\Sigma  = diag({\sigma _1},...,{\sigma _n}),\begin{array}{*{20}{c}}
{{\sigma _1} \ge {\sigma _2} \ge  \cdots  \ge {\sigma _n} \ge 0}&{}
\end{array}\]

The matrices \(U \in {R^{m \times n}}\) and \(V \in {R^{m \times n}}\) consistt
of the left and right singular vectors
\[U = ({u_1},...,{u_n})\begin{array}{*{20}{c}}
{}&{V = ({v_1},...,{v_n})}
\end{array}\]
and both matrices have orthonormal columns:
\[{U^T}U = {V^T}V = I\]

Here we apply the svd to the inverse problem \(x = {A^{ - 1}}b\), we immediately
see that the ``naive'' solution is given by
\begin{equation}
x = {A^{ - 1}}b = \sum\limits_{i = 1}^n {\frac{{u_i^Tb}}{{{\sigma _i}}}}
{v_i}
\end{equation}

\subsection{SVD Analysis and the Discrete Picard Condition}
The Discrete picard Condition: Let \(\tau \) denote the level at which the
computed singular values \({\sigma _i}\) level off due to rounding errors. The
discrete Picard condition is satisfied if, for all singular values larger than
\(\tau \), the correspoing coefficients \(\left| {u_i^Tb} \right|\), on average,
decay faster than the \({\sigma _i}\).

Figure.\ref{fig:picard_plot} shows the picard plot fro the discretized gravity
surveing problem with different noise level.
\begin{figure}
	\centering
		\begin{subfigure}[b]{0.7\textwidth}
			\includegraphics[width=\textwidth]{images/PicardPlot/picardplot_without_noise}
			\caption{With
			no noise in the right hand side (only discretization noise)}
		\end{subfigure}
		\begin{subfigure}[b]{0.7\textwidth}
			\includegraphics[width=\textwidth]{images/PicardPlot/picardplot_with_noise}
			\caption{With
			added noise in the right hand side}
		\end{subfigure}
		\caption{The Picard plots for the discretized gravity
		surveing problem with different noise level. (a) With
		no added noise (only discretization noise). (b) With
		an addional white noised added to the measured g
		(1E-4 random white noise)}\label{fig:picard_plot}
\end{figure}
\subsection{Truncated SVD}
Figure.\ref{fig:tsvd_plot} shows tsvd reconstructed f for the discretized
gravity surveing problem with differernt noise level.
\begin{figure}
	\centering
		\begin{subfigure}[b]{0.7\textwidth}
			\includegraphics[width=\textwidth]{images/TSVD/tsvdplot}
			\caption{With
			no noise in the right hand side (only discretization noise) truncated number
			k = 48}
		\end{subfigure}
		\begin{subfigure}[b]{0.7\textwidth}
			\includegraphics[width=\textwidth]{images/TSVD/tsvdplot_withnoise}
			\caption{With
			added noise in the right hand side (white noise 1E-4), truncated number k =
			18}
		\end{subfigure}
		\caption{TSVD f reconstruction}\label{fig:tsvd_plot}
\end{figure}
\subsection{Tikhonov Regularization}
Figure.\ref{fig:tikhonov_plot} shows Tikhonov Regularization reconstructed f for the
discretized gravity surveing problem with differernt noise level.
\begin{figure}
	\centering
		\begin{subfigure}[b]{0.7\textwidth}
			\includegraphics[width=\textwidth]{images/TikhonovRegularization/tikhonovplot}
			\caption{With
			no noise in the right hand side (only discretization noise), \(\lambda \) =
			1E-12}
		\end{subfigure}
		\begin{subfigure}[b]{0.7\textwidth}
			\includegraphics[width=\textwidth]{images/TikhonovRegularization/tikhonovplot_withnoise}
			\caption{With
			added noise in the right hand side (white noise 1E-4), \(\lambda \) =
			1E-3}
		\end{subfigure}
		\caption{Tikhonov Regularization f reconstruction}\label{fig:tikhonov_plot}
\end{figure}
\subsection{Fitness of the Reconstruction: Generalized Cross Validation}
The fitness is calculated as:
\begin{equation}
	\mathop {\min }\limits_\lambda  ||A{x_\lambda } - {b^{exact}}||_2^2
\end{equation}
However, we can't calculate it since \(b^{exact}\) is not available. Generalized 
Cross Validation is a classical statistical technique that comes into good use
here \cite{hansen2010discrete}.

Using Generalized Cross Validation(GSV), the fitness can be calculated as:
\begin{equation}
	\mathop {\min }\limits_\lambda  \frac{{||A{x_\lambda } - b||_2^2}}{{{{(m -
	\sum\nolimits_{i = 1}^n {\varphi _i^{[\lambda ]}} )}^2}}}
	\label{eq:disintegral}
\end{equation}
\subsection{Self Consistent Regularization With Cross Validation}

\section{Cellular Evolutionary Algorithm}
\begin{figure}
	\centering
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{images/CEA/simulation1/generation0}
			\caption{lowest fitness = 2.60e-12}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{images/CEA/simulation1/generation1}
			\caption{lowest fitness = 1.65e-25}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{images/CEA/simulation1/generation2}
			\caption{lowest fitness = 1.65e-25}
		\end{subfigure}
		\begin{subfigure}[b]{0.4\textwidth}
			\includegraphics[width=\textwidth]{images/CEA/simulation1/generation3}
			\caption{lowest fitness = 1.65e-25}
		\end{subfigure}
		\caption{4 generation of CEA simulation fitness map, we come to the exact de
		noise measurement of only 2 generation}\label{fig:ceasimulation1}
\end{figure}
\subsection{Evolutionary Algorithms}
Evolutinary algorithms (EAs) are a family of heuristic search methods that are
often used nowadays to find satisfactory solutions to difficult optimization and
machine learning problems. EAs are lossely based on a few fundamental
evolutionary ideas introduced by Darwin in the niewteenth century. These
concepts revolve around the notion of populations of organisms adapting to their
enviroment through genetic inheritance and survivial of the fittest.

Each individual may be viewed as a representation, according to an appropriate
encoding, of a particular solution to an algorithmic problem. To implement an
EAs for solving a given problem, at least approximately, the user must provide
the following pieces of information.

\subsubsection{Representation}
Individuals in the population represent solutions to a pronlem and must be
encoded in some way to manipulated by the EA processes. They may be just string
of binary digits, which is a widespread and universal representation, or they
may be any other data structure that is suitable for the problem at hand such as
string of integers, alphabetic characters, strings of real numbers, and even
trees or graphs.
\subsubsection{Genetic Operators}

Usually Evolutionary Algorithm assume that the strucutre of the population is
panmictic, which means that any inidividual may interact with any other individual in the population.
However, this need not be always the case: we often see population sin the
biological and social world in which individuals only interact with a subset of
the rest of the population. This situation can usefully be depicated by using
the concept of a population graph \cite{hoekstra2010simulating}.

\bibliography{mybib}{}
\bibliographystyle{plain}
\end{document}
